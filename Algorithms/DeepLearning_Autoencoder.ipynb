{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepLearning_Autoencoder.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "m913TteujpvI",
        "gJll9Pazj4dD"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NW7PjULhGt7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcJDer3WhZVq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = \"/content/drive/My Drive/M2/DeepLearning\"\n",
        "\n",
        "import sys\n",
        "sys.path.append(path)\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDUE3eJpjoEM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "train_images = train_images.reshape((60000, 28, 28, 1))\n",
        "test_images = test_images.reshape((10000, 28, 28, 1))\n",
        "print(train_images.shape)\n",
        "print(test_images.shape)\n",
        "\n",
        "\n",
        "import mnist_reader\n",
        "X_train, y_train = mnist_reader.load_mnist(path+'/Datasets', kind='train')\n",
        "X_test, y_test = mnist_reader.load_mnist(path+'/Datasets', kind='t10k')\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "\n",
        "X_train, X_test, train_images, test_images = X_train / 255.0, X_test / 255.0, train_images / 255.0, test_images / 255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m913TteujpvI",
        "colab_type": "text"
      },
      "source": [
        "### **1. Deep Autoencoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2t4xn_WDhH6K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n",
        "\n",
        "nmi = normalized_mutual_info_score\n",
        "ari = adjusted_rand_score\n",
        "\n",
        "\n",
        "\n",
        "#################################################\n",
        "# Hyper-parameters\n",
        "#################################################\n",
        "epoch = 200\n",
        "batch_size = 256\n",
        "\n",
        "encoding_dim = 7*7\n",
        "hidden_dim = 512\n",
        "\n",
        "activation='elu'\n",
        "optimizer='adam'\n",
        "loss='mean_squared_error'\n",
        "loss='binary_crossentropy'\n",
        "#################################################\n",
        "\n",
        "out_activation = 'sigmoid' if loss=='binary_crossentropy' else 'linear'\n",
        "if activation=='leaky_relu':\n",
        "  activation=None\n",
        "\n",
        "#################################################\n",
        "# Auto-encoder\n",
        "#################################################\n",
        "input_img = Input(shape=(784,))\n",
        "\n",
        "\n",
        "####### Encoder #######\n",
        "#   Layer\n",
        "encoded = Dense(hidden_dim, activation=activation)(input_img)\n",
        "if activation is None: encoded = layers.LeakyReLU()(encoded)\n",
        "\n",
        "#   Layer\n",
        "encoded = Dense(hidden_dim//4, activation=activation)(encoded)\n",
        "if activation is None: encoded = layers.LeakyReLU()(encoded)\n",
        "\n",
        "#   Layer\n",
        "encoded = Dense(encoding_dim, activation=out_activation)(encoded)\n",
        "\n",
        "\n",
        "####### Decoder #######\n",
        "#   Layer\n",
        "decoded = Dense(hidden_dim//4, activation=activation)(encoded)\n",
        "if activation is None: decoded = layers.LeakyReLU()(decoded)\n",
        "\n",
        "#   Layer\n",
        "decoded = Dense(hidden_dim, activation=activation)(decoded)\n",
        "if activation is None: decoded = layers.LeakyReLU()(decoded)\n",
        "\n",
        "#   Layer\n",
        "decoded = Dense(784, activation=out_activation)(decoded)\n",
        "\n",
        "\n",
        "####### Make model #######\n",
        "autoencoder = Model(input_img, decoded)\n",
        "autoencoder.compile(optimizer=optimizer, loss=loss)\n",
        "\n",
        "autoencoder.summary()\n",
        "\n",
        "\n",
        "####### Train #######\n",
        "autoencoder.fit(X_train, X_train,\n",
        "                epochs=epoch,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=True,\n",
        "                validation_data=(X_test, X_test))\n",
        "\n",
        "\n",
        "####### Encode images #######\n",
        "encoder = Model(input_img, encoded)\n",
        "encoded_images = encoder.predict(X_train)\n",
        "#################################################\n",
        "\n",
        "\n",
        "\n",
        "#################################################\n",
        "# KMeans\n",
        "#################################################\n",
        "print(\"run KMeans\")\n",
        "kmeans = KMeans(n_clusters=10, n_init=20)\n",
        "kmeans.fit(encoded_images)\n",
        "\n",
        "print(nmi(kmeans.labels_, y_train))\n",
        "print(ari(kmeans.labels_, y_train))\n",
        "#################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJll9Pazj4dD",
        "colab_type": "text"
      },
      "source": [
        "### **2. Deep Convolutional Autoencoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqLpAQewhml1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import Input, layers, models\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n",
        "from math import sqrt\n",
        "\n",
        "nmi = normalized_mutual_info_score\n",
        "ari = adjusted_rand_score\n",
        "\n",
        "\n",
        "\n",
        "#################################################\n",
        "# Hyper-parameters\n",
        "#################################################\n",
        "epoch = 100\n",
        "batch_size = 256\n",
        "\n",
        "encoding_dim = 7*7\n",
        "\n",
        "#activation='elu'\n",
        "activation='leaky_relu'\n",
        "optimizer='adam'\n",
        "#loss='mean_squared_error'\n",
        "loss='binary_crossentropy'\n",
        "#################################################\n",
        "\n",
        "encoding_size = int(sqrt(encoding_dim))\n",
        "out_activation = 'sigmoid' if loss=='binary_crossentropy' else 'linear'\n",
        "if activation=='leaky_relu':\n",
        "  activation=None\n",
        "\n",
        "#################################################\n",
        "# Convolutional Auto-encoder\n",
        "#################################################\n",
        "input_img = Input((28, 28, 1))\n",
        "\n",
        "\n",
        "####### Encoder #######\n",
        "#   Layer\n",
        "encoded = layers.Conv2D(32, (3, 3), activation=activation)(input_img)\n",
        "if activation is None: encoded = layers.LeakyReLU()(encoded)\n",
        "encoded = layers.MaxPooling2D((2, 2))(encoded)\n",
        "\n",
        "#   Layer\n",
        "encoded = layers.Conv2D(64, (3, 3), activation=activation)(encoded)\n",
        "if activation is None: encoded = layers.LeakyReLU()(encoded)\n",
        "encoded = layers.MaxPooling2D((2, 2))(encoded)\n",
        "\n",
        "#   Layer\n",
        "encoded = layers.Conv2D(64, (3, 3), activation=activation)(encoded)\n",
        "if activation is None: encoded = layers.LeakyReLU()(encoded)\n",
        "\n",
        "#   Layer\n",
        "encoded = layers.Flatten()(encoded)\n",
        "encoded = layers.Dense(encoding_dim, activation=out_activation)(encoded)\n",
        "\n",
        "\n",
        "####### Decoder #######\n",
        "decoded = layers.Reshape((encoding_size,encoding_size,1))(encoded)\n",
        "#   Layer\n",
        "decoded = layers.Conv2DTranspose(64,(3, 3), strides=2, activation=activation, padding='same')(decoded)\n",
        "if activation is None: decoded = layers.LeakyReLU()(decoded)\n",
        "\n",
        "#   Layer\n",
        "decoded = layers.Conv2DTranspose(64,(3, 3), strides=2, activation=activation, padding='same')(decoded)\n",
        "if activation is None: decoded = layers.LeakyReLU()(decoded)\n",
        "\n",
        "#   Layer\n",
        "decoded = layers.Conv2DTranspose(32,(3, 3), activation=activation, padding='same')(decoded)\n",
        "if activation is None: decoded = layers.LeakyReLU()(decoded)\n",
        "\n",
        "#   Layer\n",
        "decoded = layers.Conv2D(1, (3, 3), activation=out_activation, padding='same')(decoded)\n",
        "\n",
        "\n",
        "####### Make model #######\n",
        "autoencoder = models.Model(input_img, decoded)\n",
        "autoencoder.compile(optimizer=optimizer, loss=loss)\n",
        "\n",
        "autoencoder.summary()\n",
        "\n",
        "\n",
        "####### Train #######\n",
        "autoencoder.fit(train_images, train_images,\n",
        "                epochs=epoch,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=True,\n",
        "                validation_data=(test_images, test_images))\n",
        "\n",
        "\n",
        "####### Encode images #######\n",
        "encoder = Model(input_img, encoded)\n",
        "encoded_images = encoder.predict(train_images)\n",
        "print(encoded_images.shape)\n",
        "#################################################\n",
        "\n",
        "\n",
        "\n",
        "#################################################\n",
        "# KMeans\n",
        "#################################################\n",
        "print(\"run KMeans\")\n",
        "kmeans = KMeans(n_clusters=10, n_init=20)\n",
        "kmeans.fit(encoded_images)\n",
        "\n",
        "print(nmi(kmeans.labels_, train_labels))\n",
        "print(ari(kmeans.labels_, train_labels))\n",
        "#################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48eGo9LwkAI0",
        "colab_type": "text"
      },
      "source": [
        "### **3. Deep Convolutional Adversarial Autoencoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCwbP9WwkGUi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "from tensorflow.keras import layers, models, Input\n",
        "import numpy as np\n",
        "from statistics import mean\n",
        "\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "\n",
        "\n",
        "#################################################\n",
        "# Hyper-parameters\n",
        "#################################################\n",
        "batch_size = 256\n",
        "\n",
        "epoch = 50\n",
        "noise_dim = 100\n",
        "#################################################\n",
        "\n",
        "\n",
        "\n",
        "#################################################\n",
        "# Generator\n",
        "#################################################\n",
        "def make_generator_model():\n",
        "    input_img = Input((100, ))\n",
        "    layer = layers.Dense(7*7*256, use_bias=False)(input_img)\n",
        "    layer = layers.BatchNormalization()(layer)\n",
        "    layer = layers.LeakyReLU()(layer)\n",
        "\n",
        "    layer = layers.Reshape((7, 7, 256))(layer)\n",
        "    #assert model.output_shape == (None, 7, 7, 256) # Note: None is the batch size\n",
        "\n",
        "    layer = layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False)(layer)\n",
        "    #assert model.output_shape == (None, 7, 7, 128)(layer)\n",
        "    layer = layers.BatchNormalization()(layer)\n",
        "    layer = layers.LeakyReLU()(layer)\n",
        "\n",
        "    layer = layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False)(layer)\n",
        "    #assert model.output_shape == (None, 14, 14, 64)(layer)\n",
        "    layer = layers.BatchNormalization()(layer)\n",
        "    layer = layers.LeakyReLU()(layer)\n",
        "\n",
        "    layer = layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh')(layer)\n",
        "    #assert model.output_shape == (None, 28, 28, 1)\n",
        "\n",
        "    model = models.Model(input_img, layer)\n",
        "\n",
        "    return model\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "#################################################\n",
        "\n",
        "\n",
        "\n",
        "#################################################\n",
        "# Discriminator\n",
        "#################################################\n",
        "def make_discriminator_model():\n",
        "    input_img = Input((28, 28, 1))\n",
        "    layer = layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',)(input_img)\n",
        "    layer = layers.LeakyReLU()(layer)\n",
        "    layer = layers.Dropout(0.3)(layer)\n",
        "\n",
        "    layer = layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same')(layer)\n",
        "    layer = layers.LeakyReLU()(layer)\n",
        "    layer = layers.Dropout(0.3)(layer)\n",
        "\n",
        "    layer = layers.Flatten()(layer)\n",
        "    layer = layers.Dense(1)(layer)\n",
        "\n",
        "    model = models.Model(input_img, layer)\n",
        "\n",
        "    return model\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "#################################################    \n",
        "\n",
        "discriminator = make_discriminator_model()\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "\n",
        "generator = make_generator_model()\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "\n",
        "@tf.function\n",
        "def train_step(images):\n",
        "    noise = tf.random.normal([batch_size, noise_dim])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "      generated_images = generator(noise, training=True)\n",
        "\n",
        "      real_output = discriminator(images, training=True)\n",
        "      fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "      gen_loss = generator_loss(fake_output)\n",
        "      disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "\n",
        "    return gen_loss, disc_loss\n",
        "\n",
        "def train(dataset):\n",
        "  for ep in range(epoch):\n",
        "    print(\"epoch : \",ep, \" / \", epoch)\n",
        "    gen_losses, disc_losses = [], []\n",
        "    for it in range(int(dataset.shape[0] / batch_size)):\n",
        "      image_batch = dataset[np.random.randint(low=0,high=dataset.shape[0],size=batch_size)]\n",
        "      gen_loss, disc_loss = train_step(image_batch)\n",
        "      gen_losses.append(gen_loss.numpy())\n",
        "      disc_losses.append(disc_loss.numpy())\n",
        "\n",
        "    print(\"Generator loss     =\",mean(gen_losses), \"\\nDiscriminator loss =\",mean(disc_losses))\n",
        "\n",
        "train(train_images)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}